# Optimization-Techniques :Evaluation of Gradient Optimization Techniques for Neural Networks 
Gradient descent technique is the most common and popular method for gradient optimization of neural networks. In this paper, we test and compare the stability, speed and accuracy of the following gradient optimization techniques, viz.: a regular network without momentum, Polyak’s classical momentum, RmsProp and Adam. We analyze and discuss the theoretical convergence properties of each of these techniques. We perform tests to analyze how the techniques behave for different learning rates and mini-batch sizes. In our experiments we observe that Adam gives the best results in terms of highest speed of convergence and has an accuracy of 89.608%. Index Terms—Stochastic Gradient Descent (SGD), Polyak’s momentum, RmsProp, ADAM, mini-batch. 
