{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed node or string: <_ast.Name object at 0x000002772D229548>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-68c803744072>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-68c803744072>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m     \u001b[0mnet_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m     \u001b[0mnet_dims\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Adding the digits layer with dimensionality = 10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Network dimensions are:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[1;34m(node_or_string)\u001b[0m\n\u001b[0;32m     89\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\ast.py\u001b[0m in \u001b[0;36m_convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\ast.py\u001b[0m in \u001b[0;36m_convert_signed_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnaryOp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mUAdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUSub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0moperand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moperand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUAdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moperand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\ast.py\u001b[0m in \u001b[0;36m_convert_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'malformed node or string: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnaryOp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mUAdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUSub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: malformed node or string: <_ast.Name object at 0x000002772D229548>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from load_mnist import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import sys, ast\n",
    "import random\n",
    "import math\n",
    "epsilon = 1e-5\n",
    "import simplejson\n",
    "def relu(Z):\n",
    "    '''\n",
    "    computes relu activation of Z\n",
    "\n",
    "    Inputs:\n",
    "        Z is a numpy.ndarray (n, m)\n",
    "\n",
    "    Returns:\n",
    "        A is activation. numpy.ndarray (n, m)\n",
    "        cache is a dictionary with {\"Z\", Z}\n",
    "    '''\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = {}\n",
    "    cache[\"Z\"] = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_der(dA, cache):\n",
    "    '''\n",
    "    computes derivative of relu activation\n",
    "\n",
    "    Inputs:\n",
    "        dA is the derivative from subsequent layer. numpy.ndarray (n, m)\n",
    "        cache is a dictionary with {\"Z\", Z}, where Z was the input\n",
    "        to the activation layer during forward propagation\n",
    "\n",
    "    Returns:\n",
    "        dZ is the derivative. numpy.ndarray (n,m)\n",
    "    '''\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    Z = cache[\"Z\"]\n",
    "    dZ[Z<0] = 0\n",
    "    return dZ\n",
    "\n",
    "def linear(Z):\n",
    "    '''\n",
    "    computes linear activation of Z\n",
    "    This function is implemented for completeness\n",
    "\n",
    "    Inputs:\n",
    "        Z is a numpy.ndarray (n, m)\n",
    "\n",
    "    Returns:\n",
    "        A is activation. numpy.ndarray (n, m)\n",
    "        cache is a dictionary with {\"Z\", Z}\n",
    "    '''\n",
    "    A = Z\n",
    "    cache = {}\n",
    "    return A, cache\n",
    "\n",
    "def linear_der(dA, cache):\n",
    "    '''\n",
    "    computes derivative of linear activation\n",
    "    This function is implemented for completeness\n",
    "\n",
    "    Inputs:\n",
    "        dA is the derivative from subsequent layer. numpy.ndarray (n, m)\n",
    "        cache is a dictionary with {\"Z\", Z}, where Z was the input\n",
    "        to the activation layer during forward propagation\n",
    "\n",
    "    Returns:\n",
    "        dZ is the derivative. numpy.ndarray (n,m)\n",
    "    '''\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    return dZ\n",
    "\n",
    "def softmax_cross_entropy_loss(Z, Y=np.array([])):\n",
    "    '''\n",
    "    Computes the softmax activation of the inputs Z\n",
    "    Estimates the cross entropy loss\n",
    "\n",
    "    Inputs:\n",
    "        Z - numpy.ndarray (n, m)\n",
    "        Y - numpy.ndarray (1, m) of labels\n",
    "            when y=[] loss is set to []\n",
    "\n",
    "    Returns:\n",
    "        A - numpy.ndarray (n, m) of softmax activations\n",
    "        cache -  a dictionary to store the activations later used to estimate derivatives\n",
    "        loss - cost of prediction\n",
    "    '''\n",
    "    ### CODE HERE\n",
    "    cache = {}\n",
    "    A = np.exp(Z - np.max(Z, axis=0)) / np.sum(np.exp(Z - np.max(Z, axis=0)), axis=0, keepdims=True)\n",
    "    cache['A'] = A\n",
    "    one_hot_targets = np.array([np.eye(Z.shape[0])[int(Y[0][int(i)])] for i in range(len(Y[0]))]).T\n",
    "    loss = -np.sum(one_hot_targets * np.log(A)) / Y.shape[1]\n",
    "    return A, cache, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def softmax_cross_entropy_loss_der(Y, cache):\n",
    "    '''\n",
    "    Computes the derivative of softmax activation and cross entropy loss\n",
    "\n",
    "    Inputs:\n",
    "        Y - numpy.ndarray (1, m) of labels\n",
    "        cache -  a dictionary with cached activations A of size (n,m)\n",
    "\n",
    "    Returns:\n",
    "        dZ - numpy.ndarray (n, m) derivative for the previous layer\n",
    "    '''\n",
    "    ### CODE HERE\n",
    "    one_hot_targets = np.array([np.eye(cache['A'].shape[0])[int(Y[0][int(i)])] for i in range(Y.shape[1])]).T\n",
    "    dZ = cache['A'] - one_hot_targets\n",
    "    return dZ / cache['A'].shape[1]\n",
    "\n",
    "\n",
    "def initialize_multilayer_weights(net_dims):\n",
    "    '''\n",
    "    Initializes the weights of the multilayer network\n",
    "\n",
    "    Inputs:\n",
    "        net_dims - tuple of network dimensions\n",
    "\n",
    "    Returns:\n",
    "        dictionary of parameters\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    numLayers = len(net_dims)\n",
    "    parameters = {}\n",
    "    for l in range(numLayers-1):\n",
    "        parameters[\"W\"+str(l+1)] = np.random.randn(net_dims[l+1],net_dims[l])*0.01#CODE HERE\n",
    "        parameters[\"b\"+str(l+1)] = np.random.randn(net_dims[l+1],1)*0.01\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    '''\n",
    "    Input A propagates through the layer\n",
    "    Z = WA + b is the output of this layer.\n",
    "\n",
    "    Inputs:\n",
    "        A - numpy.ndarray (n,m) the input to the layer\n",
    "        W - numpy.ndarray (n_out, n) the weights of the layer\n",
    "        b - numpy.ndarray (n_out, 1) the bias of the layer\n",
    "\n",
    "    Returns:\n",
    "        Z = WA + b, where Z is the numpy.ndarray (n_out, m) dimensions\n",
    "        cache - a dictionary containing the inputs A\n",
    "    '''\n",
    "    ### CODE HERE\n",
    "    Z = np.dot(W,A) + b\n",
    "    cache = {}\n",
    "    cache[\"A\"] = A\n",
    "    return Z, cache\n",
    "\n",
    "def layer_forward(A_prev, W, b, activation):\n",
    "    '''\n",
    "    Input A_prev propagates through the layer and the activation\n",
    "\n",
    "    Inputs:\n",
    "        A_prev - numpy.ndarray (n,m) the input to the layer\n",
    "        W - numpy.ndarray (n_out, n) the weights of the layer\n",
    "        b - numpy.ndarray (n_out, 1) the bias of the layer\n",
    "        activation - is the string that specifies the activation function\n",
    "\n",
    "    Returns:\n",
    "        A = g(Z), where Z = WA + b, where Z is the numpy.ndarray (n_out, m) dimensions\n",
    "        g is the activation function\n",
    "        cache - a dictionary containing the cache from the linear and the nonlinear propagation\n",
    "        to be used for derivative\n",
    "    '''\n",
    "    Z, lin_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"relu\":\n",
    "        A, act_cache = relu(Z)\n",
    "    elif activation == \"linear\":\n",
    "        A, act_cache = linear(Z)\n",
    "\n",
    "    cache = {}\n",
    "    cache[\"lin_cache\"] = lin_cache\n",
    "    cache[\"act_cache\"] = act_cache\n",
    "    return A, cache\n",
    "\n",
    "def multi_layer_forward(X, parameters):\n",
    "    '''\n",
    "    Forward propgation through the layers of the network\n",
    "\n",
    "    Inputs:\n",
    "        X - numpy.ndarray (n,m) with n features and m samples\n",
    "        parameters - dictionary of network parameters {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
    "    Returns:\n",
    "        AL - numpy.ndarray (c,m)  - outputs of the last fully connected layer before softmax\n",
    "            where c is number of categories and m is number of samples in the batch\n",
    "        caches - a dictionary of associated caches of parameters and network inputs\n",
    "    '''\n",
    "    L = len(parameters)//2\n",
    "    A = X\n",
    "    caches = []\n",
    "    for l in range(1,L):  # since there is no W0 and b0\n",
    "        A, cache = layer_forward(A, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = layer_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"linear\")\n",
    "    caches.append(cache)\n",
    "    return AL, caches\n",
    "\n",
    "def linear_backward(dZ, cache, W, b):\n",
    "    '''\n",
    "    Backward prpagation through the linear layer\n",
    "\n",
    "    Inputs:\n",
    "        dZ - numpy.ndarray (n,m) derivative dL/dz\n",
    "        cache - a dictionary containing the inputs A, for the linear layer\n",
    "            where Z = WA + b,\n",
    "            Z is (n,m); W is (n,p); A is (p,m); b is (n,1)\n",
    "        W - numpy.ndarray (n,p)\n",
    "        b - numpy.ndarray (n, 1)\n",
    "\n",
    "    Returns:\n",
    "        dA_prev - numpy.ndarray (p,m) the derivative to the previous layer\n",
    "        dW - numpy.ndarray (n,p) the gradient of W\n",
    "        db - numpy.ndarray (n, 1) the gradient of b\n",
    "    '''\n",
    "    A_prev = cache[\"A\"]\n",
    "    ## CODE HERE\n",
    "    dW = np.dot(dZ,A_prev.T)\n",
    "    db = np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db\n",
    "def tanh_der(dA, cache):\n",
    "    '''\n",
    "    computes derivative of tanh activation\n",
    "\n",
    "    Inputs:\n",
    "        dA is the derivative from subsequent layer. numpy.ndarray (n, m)\n",
    "        cache is a dictionary with {\"Z\", Z}, where Z was the input\n",
    "        to the activation layer during forward propagation\n",
    "\n",
    "    Returns:\n",
    "        dZ is the derivative. numpy.ndarray (n,m)\n",
    "    '''\n",
    "    ### CODE HERE\n",
    "    Z=cache[\"Z\"]\n",
    "    dZ=1.0-np.tanh(Z)**2\n",
    "\n",
    "\n",
    "    return dZ\n",
    "def sigmoid_der(dA, cache):\n",
    "    '''\n",
    "    computes derivative of sigmoid activation\n",
    "\n",
    "    Inputs:\n",
    "        dA is the derivative from subsequent layer. numpy.ndarray (n, m)\n",
    "        cache is a dictionary with {\"Z\", Z}, where Z was the input\n",
    "        to the activation layer during forward propagation\n",
    "\n",
    "    Returns:\n",
    "        dZ is the derivative. numpy.ndarray (n,m)\n",
    "    '''\n",
    "\n",
    "    ### CODE HERE\n",
    "    #A,third_cache=sigmoid(cache[\"Z\"])\n",
    "    #D=cache[\"Z\"]\n",
    "    #A=1/(1+np.exp(-D))\n",
    "    #mul=np.multiply(A,(1-A))\n",
    "    #dZ=np.multiply(dA,mul)\n",
    "    dZ=np.multiply(dA,np.multiply(sigmoid(cache[\"Z\"])[0],1-sigmoid(cache[\"Z\"])[0]))\n",
    "    return dZ\n",
    "def sigmoid(Z):\n",
    "    '''\n",
    "    computes sigmoid activation of Z\n",
    "\n",
    "    Inputs:\n",
    "        Z is a numpy.ndarray (n, m)\n",
    "\n",
    "    Returns:\n",
    "        A is activation. numpy.ndarray (n, m)\n",
    "        cache is a dictionary with {\"Z\", Z}\n",
    "    '''\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = {}\n",
    "    cache[\"Z\"] = Z\n",
    "    return A, cache\n",
    "def tanh(Z):\n",
    "    '''\n",
    "    computes tanh activation of Z\n",
    "\n",
    "    Inputs:\n",
    "        Z is a numpy.ndarray (n, m)\n",
    "\n",
    "    Returns:\n",
    "        A is activation. numpy.ndarray (n, m)\n",
    "        cache is a dictionary with {\"Z\", Z}\n",
    "    '''\n",
    "    A = np.tanh(Z)\n",
    "    cache = {}\n",
    "    cache[\"Z\"] = Z\n",
    "    return A, cache\n",
    "\n",
    "def layer_backward(dA, cache, W, b, activation):\n",
    "    '''\n",
    "    Backward propagation through the activation and linear layer\n",
    "\n",
    "    Inputs:\n",
    "        dA - numpy.ndarray (n,m) the derivative to the previous layer\n",
    "        cache - dictionary containing the linear_cache and the activation_cache\n",
    "        activation - activation of the layer\n",
    "        W - numpy.ndarray (n,p)\n",
    "        b - numpy.ndarray (n, 1)\n",
    "\n",
    "    Returns:\n",
    "        dA_prev - numpy.ndarray (p,m) the derivative to the previous layer\n",
    "        dW - numpy.ndarray (n,p) the gradient of W\n",
    "        db - numpy.ndarray (n, 1) the gradient of b\n",
    "    '''\n",
    "    lin_cache = cache[\"lin_cache\"]\n",
    "    act_cache = cache[\"act_cache\"]\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_der(dA, act_cache)\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_der(dA, act_cache)\n",
    "    elif activation == \"relu\":\n",
    "        dZ = relu_der(dA, act_cache)\n",
    "    elif activation == \"linear\":\n",
    "        dZ = linear_der(dA, act_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, lin_cache, W, b)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def multi_layer_backward(dAL, caches, parameters):\n",
    "    '''\n",
    "    Back propgation through the layers of the network (except softmax cross entropy)\n",
    "    softmax_cross_entropy can be handled separately\n",
    "\n",
    "    Inputs:\n",
    "        dAL - numpy.ndarray (n,m) derivatives from the softmax_cross_entropy layer\n",
    "        caches - a dictionary of associated caches of parameters and network inputs\n",
    "        parameters - dictionary of network parameters {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
    "\n",
    "    Returns:\n",
    "        gradients - dictionary of gradient of network parameters\n",
    "            {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...}\n",
    "    '''\n",
    "    L = len(caches)  # with one hidden layer, L = 2\n",
    "    gradients = {}\n",
    "    dA = dAL\n",
    "    activation = \"linear\"\n",
    "    for l in reversed(range(1,L+1)):\n",
    "        dA, gradients[\"dW\"+str(l)], gradients[\"db\"+str(l)] = \\\n",
    "                    layer_backward(dA, caches[l-1], \\\n",
    "                    parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],\\\n",
    "                    activation)\n",
    "        activation = \"relu\"\n",
    "    return gradients\n",
    "\n",
    "def classify(X,Y, parameters):\n",
    "    '''\n",
    "    Network prediction for inputs X\n",
    "\n",
    "    Inputs:\n",
    "        X - numpy.ndarray (n,m) with n features and m samples\n",
    "        parameters - dictionary of network parameters\n",
    "            {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "    Returns:\n",
    "        YPred - numpy.ndarray (1,m) of predictions\n",
    "    '''\n",
    "    ### CODE HERE\n",
    "    # Forward propagate X using multi_layer_forward\n",
    "    # Get predictions using softmax_cross_entropy_loss\n",
    "    # Estimate the class labels using predictions\n",
    "    AL, caches = multi_layer_forward(X, parameters)\n",
    "    Ypred, cache, cost = softmax_cross_entropy_loss(AL, Y)\n",
    "    Ypred = np.argmax(Ypred, axis=0)\n",
    "    return Ypred\n",
    "def update_parameters(parameters, gradients,learning_rate, decay_rate=0.0):\n",
    "    '''\n",
    "    Updates the network parameters with gradient descent\n",
    "\n",
    "    Inputs:\n",
    "        parameters - dictionary of network parameters\n",
    "            {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "        gradients - dictionary of gradient of network parameters\n",
    "            {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...}\n",
    "        epoch - epoch number\n",
    "        learning_rate - step size for learning\n",
    "        decay_rate - rate of decay of step size - not necessary - in case you want to use\n",
    "    '''\n",
    "    alpha=learning_rate\n",
    "\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    ### CODE HERE\n",
    "    parameters3={}\n",
    "    for l in range(1,L+1):\n",
    "        #parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - alpha * gradients[\"dW\"+str(l)]\n",
    "        #parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - alpha * gradients[\"db\"+str(l)]\n",
    "        parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - alpha * gradients[\"dW\"+str(l)]\n",
    "        parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - alpha * gradients[\"db\"+str(l)]\n",
    "    #parameters=parameters3.copy()\n",
    "    #parameters3.clear()\n",
    "\n",
    "    return parameters, alpha\n",
    "\n",
    "def multi_layer_network(X, Y, valid_x, valid_y,parameters, net_dims, num_iterations=500, learning_rate=0.2, decay_rate=0.01):\n",
    "\n",
    "    #parameters = initialize_multilayer_weights(net_dims)\n",
    "\n",
    "    A0 = X\n",
    "    #L = len(parameters)//2\n",
    "    #print(parameters[\"W1\"][0])\n",
    "\n",
    "\n",
    "    A_f,cache_f = multi_layer_forward(A0,parameters)\n",
    "    ## call to softmax cross entropy loss\n",
    "    Asoft,cache_soft,loss = softmax_cross_entropy_loss(A_f,Y)\n",
    "\n",
    "    #VALIDATION\n",
    "    V_f,_ = multi_layer_forward(valid_x,parameters)\n",
    "    ## call to softmax cross entropy loss\n",
    "    _,_,v_loss = softmax_cross_entropy_loss(V_f,valid_y)\n",
    "\n",
    "\n",
    "\n",
    "    # Backward Prop\n",
    "    ## call to softmax cross entropy loss der\n",
    "    dZ = softmax_cross_entropy_loss_der(Y,cache_soft)\n",
    "    ## call to multi_layer_backward to get gradients\n",
    "    gradients = multi_layer_backward(dZ,cache_f,parameters)\n",
    "    ## call to update the parameters\n",
    "    #parameters, alpha = update_parameters(parameters,gradients,ii,learning_rate,decay_rate)\n",
    "    parameters, alpha = update_parameters(parameters,gradients,learning_rate,decay_rate)\n",
    "    #parameters=parameters4.copy()\n",
    "    #parameters4.clear()\n",
    "    print(\"Cost at iteration is: %.05f, learning rate: %.05f\" %(loss, alpha))\n",
    "    print(\"Cost at iteration is: %.05f, learning rate: %.05f\" %(v_loss, alpha))\n",
    "\n",
    "\n",
    "\n",
    "        # Forward Prop\n",
    "\n",
    "        ## call to multi_layer_forward to get activations\n",
    "\n",
    "    #print(\"fff\",parameters[\"W1\"][0])\n",
    "    return loss, parameters, v_loss\n",
    "\n",
    "def get_mini_batches(X,Y,mini_batch_size,seed=0):\n",
    "    np.random.seed(seed)\n",
    "    mini_batches=[]\n",
    "    m=X.shape[1]\n",
    "    #step1 shuffle X and Y\n",
    "    permutation=list(np.random.permutation(m))\n",
    "    shuffled_X=X[:,permutation]\n",
    "    shuffled_Y=Y[:,permutation].reshape((1,m))\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    net_dims = ast.literal_eval( sys.argv[1] )\n",
    "    net_dims.append(10) # Adding the digits layer with dimensionality = 10\n",
    "    print(\"Network dimensions are:\" + str(net_dims))\n",
    "\n",
    "    # getting the subset dataset from MNIST\n",
    "    Original_train_data, Original_train_label, test_data, test_label = \\\n",
    "            mnist(noTrSamples=60000,noTsSamples=10000,\\\n",
    "            digit_range=[0,1,2,3,4,5,6,7,8,9],\\\n",
    "            noTrPerClass=6000, noTsPerClass=1000)\n",
    "\n",
    "    train_data = Original_train_data[:,:5000]\n",
    "    train_label = Original_train_label[:,:5000]\n",
    "    validation_data = Original_train_data[:,5000:6000]\n",
    "    validation_label = Original_train_label[:,5000:6000]\n",
    "\n",
    "    for i in range(6000, 60000, 6000):\n",
    "        train_data = np.hstack((train_data,Original_train_data[:,i:i+5000]))\n",
    "        train_label = np.hstack((train_label,Original_train_label[:,i:i+5000]))\n",
    "        validation_data = np.hstack((validation_data,Original_train_data[:,i+5000:i+6000]))\n",
    "        validation_label = np.hstack((validation_label,Original_train_label[:,i+5000:i+6000]))\n",
    "    print(train_data.shape)\n",
    "    print(validation_data.shape)\n",
    "    mini_batches_train=[]\n",
    "    mini_batches_val=[]\n",
    "    for j in range(10):\n",
    "        x=get_mini_batches(train_data,train_label,500)\n",
    "        for i in range(len(x)):\n",
    "            mini_batches_train.append(x[i])\n",
    "        y=get_mini_batches(validation_data,validation_label,100)\n",
    "        for i in range(len(y)):\n",
    "            mini_batches_val.append(y[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(len(mini_batches_train))\n",
    "    print(len(mini_batches_val))\n",
    "\n",
    "    learning_rate = 0.1\n",
    "    num_iterations = 100\n",
    "    num_iter=1\n",
    "    minibatchsize_train=500\n",
    "    costs=[]\n",
    "    valid_costs=[]\n",
    "    parameters= initialize_multilayer_weights(net_dims)\n",
    "    for i in range(len(mini_batches_train)):\n",
    "        loss, parameters, v_loss = multi_layer_network(mini_batches_train[i][0],mini_batches_train[i][1],mini_batches_val[i][0],mini_batches_val[i][1],parameters,net_dims, \\\n",
    "                num_iterations=num_iterations, learning_rate=learning_rate,decay_rate=0.01)\n",
    "        #parameters=parameters2.copy()\n",
    "        #parameters2.clear()\n",
    "        if i%10 ==0:\n",
    "            costs.append(loss)\n",
    "            valid_costs.append(v_loss)\n",
    "\n",
    "    with open('output_nomomentum.txt', 'w') as f:\n",
    "        for item in valid_costs:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "    min_train_loss=min(costs)\n",
    "    print(\"min_train_loss\",min_train_loss)\n",
    "    train_Pred = classify(train_data,train_label, parameters)\n",
    "    test_Pred = classify(test_data,test_label, parameters)\n",
    "    valid_Pred = classify(validation_data,validation_label, parameters)\n",
    "\n",
    "    trAcc = 100 * (float(np.sum(train_Pred == train_label))/train_label.shape[1])\n",
    "    teAcc = 100 * (float(np.sum(test_Pred == test_label))/test_label.shape[1])\n",
    "    VaAcc = 100 * (float(np.sum(valid_Pred == validation_label))/validation_label.shape[1])\n",
    "\n",
    "    print(\"Accuracy for training set is {0:0.3f} %\".format(trAcc))\n",
    "    print(\"Accuracy for testing set is {0:0.3f} %\".format(teAcc))\n",
    "    print(\"Accuracy for Validation set is {0:0.3f} %\".format(VaAcc))\n",
    "\n",
    "    ### CODE HERE to plot costs\n",
    "    plt.plot(costs)\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    plt.plot(valid_costs)\n",
    "    plt.ylabel('costs')\n",
    "    plt.xlabel('iterations (multiples of 10)')\n",
    "    plt.title(\"Validation and training costs vs iterations at learning rate 0.2\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Test Error:\",1-(teAcc/100))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
